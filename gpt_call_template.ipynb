{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4879bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31481",
   "metadata": {},
   "source": [
    "# API Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.envからAPIキーを読む準備\n",
    "load_dotenv('./.env', override=True)\n",
    "API_VERSION = \"2024-12-01-preview\" #Azure openAI API version\n",
    "'''\n",
    "使用可能なモデル\n",
    "gpt-5-mini: reasoning(high), input(text), output(text,image), description(https://platform.openai.com/docs/models/gpt-5-mini)\n",
    "text-embedding-3-large: embedding model(https://platform.openai.com/docs/models/text-embedding-3-large)\n",
    "'''\n",
    "model_list = ['gpt-5-mini', 'text-embedding-3-large']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d66197",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8c30fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Azure openAI API クライアントの作成\n",
    "client = AzureOpenAI(\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1b554",
   "metadata": {},
   "source": [
    "# GPT5-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GPT5-miniに渡す入力\n",
    "System prompt\n",
    "Userの入力(text,image)\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and professional data scientist.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\":\"Explain me the GPT-oss model.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\":{\n",
    "                        \"url\": \"https://substackcdn.com/image/fetch/$s_!PKaP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe804b20e-7196-4529-9ca1-13a946123c7c_1589x734.png\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0faea029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a concise, practical explanation of the GPT-OSS architecture shown in your diagrams (two sizes: 20B and 120B).\n",
      "\n",
      "High-level idea\n",
      "- GPT-OSS is a Transformer-based LM that uses Mixture-of-Experts (MoE) in the feed‑forward layers to massively increase model capacity while keeping per‑token compute/memory much lower than a dense model with the same total parameter count.\n",
      "- It’s also optimized for very long contexts (≈131k tokens) and uses several modern Transformer improvements: rotary position embeddings (RoPE), grouped query attention (GQA), RMSNorm, and a SwiGLU-style feed‑forward block (inside each expert).\n",
      "\n",
      "Core components and data flow (one transformer block)\n",
      "1. Token embedding layer → token vectors (embedding dim = 2,880).\n",
      "2. Grouped Query Attention (GQA)\n",
      "   - Multi‑head attention variant that groups queries to reduce KV/attention cost so you can scale heads without the full quadratic attention overhead.\n",
      "   - Uses RoPE (rotary position embeddings) for positional information, enabling very long context lengths (supported ≈131k tokens in these diagrams).\n",
      "   - RMSNorm 1 is applied before/after as part of the block normalizations.\n",
      "3. MoE feed‑forward sublayer (replaces dense FFN)\n",
      "   - A router chooses a small number (top-k) of experts for each token; the chosen experts process the token in parallel and results are combined.\n",
      "   - Each expert uses a SwiGLU-style FFN (linear → SiLU gate → linear → multiply) with an intermediate projection size shown as 2,880 for the expert input/projection on these diagrams.\n",
      "   - RMSNorm 2 is used inside the block around the MoE.\n",
      "4. Residual connections and a final RMSNorm, then linear output layer to project to the vocabulary (vocab size ≈200k).\n",
      "\n",
      "Key numbers from the diagrams\n",
      "- Embedding dimension: 2,880\n",
      "- Vocabulary: ~200k tokens\n",
      "- Context length supported: ~131k tokens\n",
      "- Attention heads: 64\n",
      "- Activation in FFN: SwiGLU (SiLU gating)\n",
      "- Normalization: RMSNorm\n",
      "- Position encoding: RoPE\n",
      "\n",
      "Differences between the two sizes shown\n",
      "- GPT-OSS 20B\n",
      "  - Depth: 24 layers (blocks)\n",
      "  - MoE: 32 experts total (diagram)\n",
      "  - Active experts per token: 4\n",
      "  - Only ~3.6B parameters are actually active per inference step (resource savings vs all 20B being active)\n",
      "- GPT-OSS 120B\n",
      "  - Depth: 36 layers\n",
      "  - MoE: 128 experts total (diagram)\n",
      "  - Active experts per token: 4\n",
      "  - Only ~5.1B parameters are active per inference step (despite the model having 120B total params)\n",
      "\n",
      "Why MoE?\n",
      "- MoE increases representational capacity (more total parameters / experts) while keeping per‑token compute and memory much lower because only a small number of experts are executed per token.\n",
      "- This lets you build a very large model (120B params) but run inference with compute similar to a much smaller dense model (because only a few experts are used per token).\n",
      "\n",
      "Practical tradeoffs and considerations\n",
      "- Pros:\n",
      "  - Much larger effective capacity for the same or lower per‑token compute.\n",
      "  - Efficient long-context handling (RoPE + attention/grouper optimizations).\n",
      "  - State‑of‑the‑art layers (SwiGLU, RMSNorm) for stability and performance.\n",
      "- Cons / engineering complexity:\n",
      "  - MoE adds routing overhead and complexity in training/inference (synchronization, expert load balancing, memory layout).\n",
      "  - Experts can be under‑utilized if routing is poor; training techniques (load balancing losses, capacity factors) are needed.\n",
      "  - Larger total parameter count increases storage and distributed training complexity even if not all parameters are active every step.\n",
      "\n",
      "Short summary\n",
      "GPT-OSS is a long‑context Transformer that combines grouped query attention and RoPE for efficient attention at very long sequence lengths, and uses a Mixture‑of‑Experts architecture (SwiGLU experts, RMSNorm) so it can present very large model capacity (20B or 120B total params) while activating only a small fraction of parameters per token (4 experts active), giving big capacity gains with much lower per‑token compute than a fully dense model of the same total size.\n"
     ]
    }
   ],
   "source": [
    "#Azure openAI API を呼び出す\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages, #入力\n",
    "    max_completion_tokens=12800, #最大トークン数\n",
    "    model=model_list[0] #モデル選択\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "793011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion_tokens=1575, prompt_tokens=1178, total_tokens=2753\n",
      "reasoning_tokens=640\n"
     ]
    }
   ],
   "source": [
    "print(f'completion_tokens={response.usage.completion_tokens}, prompt_tokens={response.usage.prompt_tokens}, total_tokens={response.usage.total_tokens}')\n",
    "print(f'reasoning_tokens={response.usage.completion_tokens_details.reasoning_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe7a32",
   "metadata": {},
   "source": [
    "# text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de452b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=1024, [0.030616212636232376, -0.0028326271567493677, ..., -0.005696623120456934, 0.018194060772657394]\n",
      "data[1]: length=1024, [0.016034524887800217, 0.00731195043772459, ..., 0.004383934661746025, 0.023255884647369385]\n",
      "data[2]: length=1024, [0.02254379168152809, -0.002585632260888815, ..., -0.005732203833758831, 0.012679222971200943]\n",
      "Usage(prompt_tokens=6, total_tokens=6)\n",
      "embeddings shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "dimensions = 1024 #最大の埋め込み次元数\n",
    "input_text = [\"first phrase\",\"second phrase\",\"third phrase\"] #インプット\n",
    "response = client.embeddings.create(\n",
    "    input=input_text,\n",
    "    dimensions=dimensions,\n",
    "    model=model_list[1] #モデル選択\n",
    ")\n",
    "\n",
    "embeddings = np.zeros((len(input_text),dimensions)) #Embedding vectorを入れるためのnumpy配列\n",
    "\n",
    "#埋め込み結果を表示\n",
    "for i,item in enumerate(response.data):\n",
    "    length = len(item.embedding)\n",
    "    embeddings[i,:] = item.embedding\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)\n",
    "print(f'embeddings shape: {embeddings.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interngpt5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
