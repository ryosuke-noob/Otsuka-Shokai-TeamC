{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4879bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31481",
   "metadata": {},
   "source": [
    "# API Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17d3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.envからAPIキーを読む準備\n",
    "load_dotenv('./.env', override=True)\n",
    "API_VERSION = \"2024-12-01-preview\" #Azure openAI API version\n",
    "'''\n",
    "使用可能なモデル\n",
    "gpt-5-mini: reasoning(high), input(text), output(text,image), description(https://platform.openai.com/docs/models/gpt-5-mini)\n",
    "text-embedding-3-large: embedding model(https://platform.openai.com/docs/models/text-embedding-3-large)\n",
    "'''\n",
    "model_list = ['gpt-5-mini', 'text-embedding-3-large']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d66197",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c30fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Azure openAI API クライアントの作成\n",
    "client = AzureOpenAI(\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1b554",
   "metadata": {},
   "source": [
    "# GPT5-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf7c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GPT5-miniに渡す入力\n",
    "System prompt\n",
    "Userの入力(text,image)\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and professional data scientist.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\":\"Explain me the GPT-oss model.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\":{\n",
    "                        \"url\": \"https://substackcdn.com/image/fetch/$s_!PKaP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe804b20e-7196-4529-9ca1-13a946123c7c_1589x734.png\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0faea029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a clear, compact explanation of the GPT-OSS architecture shown in your diagram and what the design choices mean.\n",
      "\n",
      "High-level architecture (common to both sizes)\n",
      "- Transformer decoder stack:\n",
      "  - Token embedding layer -> repeated Transformer blocks -> final RMSNorm -> linear output logits.\n",
      "  - Each Transformer block contains:\n",
      "    - RMSNorm 1 -> Grouped Query Attention (GQA) -> residual add\n",
      "    - RMSNorm 2 -> MoE-based feed‑forward -> residual add\n",
      "- Positional encoding: RoPE (rotary position embeddings).\n",
      "- Normalization: RMSNorm (no bias, root-mean-square normalization).\n",
      "- FFN nonlinearity: SwiGLU (gated SiLU variant: two linear projections, SiLU gating, then multiplication).\n",
      "\n",
      "Key numeric specs (from the diagrams)\n",
      "- Vocabulary size: ~200k tokens.\n",
      "- Embedding / model dimension: 2,880.\n",
      "- Attention heads: 64.\n",
      "- Supported context length: 131k tokens (long-context design).\n",
      "- Feed-forward (expert) projection dims: input expert size 2,880, intermediate projection size 2,880.\n",
      "\n",
      "Two model sizes pictured\n",
      "- GPT-OSS 20B\n",
      "  - Depth: 24 transformer layers.\n",
      "  - Total parameters: ~20B.\n",
      "  - MoE expert count: 32 experts per MoE layer.\n",
      "  - Active experts per token: 4 (top-k routing).\n",
      "  - Active parameters per inference step (approx): 3.6B.\n",
      "- GPT-OSS 120B\n",
      "  - Depth: 36 transformer layers.\n",
      "  - Total parameters: ~120B.\n",
      "  - MoE expert count: 128 experts per MoE layer.\n",
      "  - Active experts per token: 4 (top-k routing).\n",
      "  - Active parameters per inference step (approx): 5.1B.\n",
      "\n",
      "Mixture-of-Experts (MoE) details and benefits\n",
      "- Each FFN layer is replaced (or augmented) by an MoE: many expert FFNs exist but only a few are used per token.\n",
      "- Routing: a router network (gating) chooses the top-k experts for each token (here k = 4).\n",
      "- Resource savings: because only 4 experts are active per token, inference memory/compute footprint is much smaller than running a dense FFN with all experts enabled. That’s how a 120B-capacity model can have a much lower active-cost per token (≈5.1B parameters active) than the full parameter count.\n",
      "- Training considerations: routers are trained to distribute load; auxiliary balancing losses are typically used to avoid collapsed routing; sparse updates/communications add engineering complexity.\n",
      "\n",
      "Grouped Query Attention (GQA)\n",
      "- GQA is an attention variant that groups queries so attention computation and memory are reduced compared to fully dense multi-head attention—useful for very long contexts. The idea is to split/query heads into groups and compute attention in a grouped way to reduce quadratic cost and memory without large quality loss.\n",
      "\n",
      "RoPE (rotary positional embeddings)\n",
      "- RoPE encodes position information by rotating query/key vector subspaces. It tends to allow better extrapolation to longer contexts (when paired with appropriate scaling/architectural choices) and is a common choice for long-context LLMs.\n",
      "\n",
      "SwiGLU FFN\n",
      "- The per-expert feed-forward uses SwiGLU: two linear layers producing two vectors, a SiLU activation on one, then elementwise multiply — this is a parameter-efficient and performant variant of the standard GELU-based FFN.\n",
      "\n",
      "Practical implications, trade-offs and use-cases\n",
      "- Pros:\n",
      "  - Very large capacity (120B parameters) for modeling power, while keeping inference compute/memory lower via sparsity (MoE).\n",
      "  - Supports very long context windows (131k tokens) useful for long documents, retrieval augmentation, multi-document summarization, codebases, etc.\n",
      "  - GQA and RoPE are design choices aimed at reducing attention cost and handling long sequences.\n",
      "- Cons / engineering complexity:\n",
      "  - MoE adds router complexity, possible load imbalance, and more complex distributed training and serving infrastructure.\n",
      "  - Sparse routing can introduce latency overheads and fragmentation in GPU usage depending on batching and deployment.\n",
      "  - Larger-capacity models still require careful evaluation for reliability, alignment, and safety.\n",
      "\n",
      "Short summary\n",
      "- GPT-OSS is a long-context GPT-style decoder with Mixture-of-Experts FFNs and grouped-query attention. The MoE design lets the family scale to very large parameter counts (20B and 120B shown) while keeping the active compute at inference down because only a handful of experts are used per token. Key building blocks are RoPE, RMSNorm, GQA, and SwiGLU FFNs, yielding a model targeted at high capacity + long-context tasks with an inference cost that’s much lower than a dense model of the same total size.\n",
      "\n",
      "If you want, I can:\n",
      "- Compare this architecture to a dense GPT (no MoE) and show the cost differences.\n",
      "- Explain the router (top-k gating) math and balancing loss in more detail.\n",
      "- Sketch how to deploy such a model efficiently for inference (batching, shard placement, sparsity-aware kernels).\n"
     ]
    }
   ],
   "source": [
    "#Azure openAI API を呼び出す\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages, #入力\n",
    "    max_completion_tokens=12800, #最大トークン数\n",
    "    model=model_list[0] #モデル選択\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion_tokens=1966, prompt_tokens=1178, total_tokens=3144\n",
      "reasoning_tokens=896\n"
     ]
    }
   ],
   "source": [
    "print(f'completion_tokens={response.usage.completion_tokens}, prompt_tokens={response.usage.prompt_tokens}, total_tokens={response.usage.total_tokens}')\n",
    "print(f'reasoning_tokens={response.usage.completion_tokens_details.reasoning_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe7a32",
   "metadata": {},
   "source": [
    "# text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de452b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=1024, [0.030590569600462914, -0.0028608080465346575, ..., -0.005655741784721613, 0.01819373480975628]\n",
      "data[1]: length=1024, [0.01606053113937378, 0.007247298490256071, ..., 0.004328966606408358, 0.02325606346130371]\n",
      "data[2]: length=1024, [0.022610539570450783, -0.002555800834670663, ..., -0.0057123806327581406, 0.012646234594285488]\n",
      "Usage(prompt_tokens=6, total_tokens=6)\n",
      "embeddings shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "dimensions = 1024 #最大の埋め込み次元数\n",
    "input_text = [\"first phrase\",\"second phrase\",\"third phrase\"] #インプット\n",
    "response = client.embeddings.create(\n",
    "    input=input_text,\n",
    "    dimensions=dimensions,\n",
    "    model=model_list[1] #モデル選択\n",
    ")\n",
    "\n",
    "embeddings = np.zeros((len(input_text),dimensions)) #Embedding vectorを入れるためのnumpy配列\n",
    "\n",
    "#埋め込み結果を表示\n",
    "for i,item in enumerate(response.data):\n",
    "    length = len(item.embedding)\n",
    "    embeddings[i,:] = item.embedding\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)\n",
    "print(f'embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e334d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
